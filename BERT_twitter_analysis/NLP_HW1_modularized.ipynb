{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1_restart.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIP2lBOTZtzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8624ec2c-24cd-4e82-88ec-706a36a3412e"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# MOUNT google drive onto COLAB notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# install and import BertTokenizer\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# import tensor datasets for creating batches of the data\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# define seed for training\n",
        "SEED = 2021\n",
        "\n",
        "# Set seed for all randoms\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sKfBBVCiEDa"
      },
      "source": [
        "# load dataset and select 80k samples with SEED\n",
        "df = pd.read_csv(\"drive/MyDrive/training16M.csv\", encoding = \"ISO-8859-1\", header=None)\n",
        "df.columns=[\"LABEL\", \"timestamp\", \"year\", \"query\", \"userid\", \"TEXT\"]\n",
        "df = df.sample(n=80000, random_state=SEED)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd27ZQMLik5F"
      },
      "source": [
        "# import re\n",
        "def tokenize_truncate(sentence, tokenizer, max_input_length):  \n",
        "    # sentence= re.sub(r\"(@[A-Za-z0â€“9_]+)|[^\\w\\s]|#|http\\S+\", \"\", sentence)\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = [tokenizer.cls_token] + tokens[:max_input_length-2] + [tokenizer.sep_token]\n",
        "    return tokens\n",
        "\n",
        "# Create BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "max_input_length=64\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(tokenize_truncate(sent, tokenizer, max_input_length)) for sent in df.TEXT.values]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiGBGD3bt95O"
      },
      "source": [
        "def get_data_tensors(input_ids, labels, train_test_ratio=0.375, train_valid_ratio=0.3, max_input_length=64):\n",
        "    # Split data into training, testing and validation sets\n",
        "    train_text, test_inputs, train_label, test_labels = train_test_split(input_ids, labels, \n",
        "                                                                random_state=SEED, test_size=train_test_ratio)\n",
        "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_text, train_label, \n",
        "                                                                random_state=SEED, test_size=train_valid_ratio)\n",
        "\n",
        "    # Pad data into arrays of length max_input_length\n",
        "    train_inputs = pad_sequences(train_inputs, maxlen=max_input_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    validation_inputs = pad_sequences(validation_inputs, maxlen=max_input_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    test_inputs = pad_sequences(test_inputs, maxlen=max_input_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # Convert inputs and labels into Long and Float tensors\n",
        "    train_inputs = torch.LongTensor(train_inputs)\n",
        "    validation_inputs = torch.LongTensor(validation_inputs)\n",
        "    train_labels = torch.FloatTensor(train_labels.to_list())\n",
        "    validation_labels = torch.FloatTensor(validation_labels.to_list())\n",
        "\n",
        "    test_inputs = torch.LongTensor(test_inputs)\n",
        "    test_labels = torch.FloatTensor(test_labels.to_list())\n",
        "    return train_inputs, train_labels,validation_inputs, validation_labels, test_inputs, test_labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVtosf_QwXxH"
      },
      "source": [
        "# Convert Tensors to batches\n",
        "def get_loaders(train_inputs, train_labels, validation_data, validation_labels):\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    validation_data = TensorDataset(validation_inputs, validation_labels)\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "    validation_loader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "    return train_loader, validation_loader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2b7YFpVuTbg"
      },
      "source": [
        "class Feedforward(torch.nn.Module):\n",
        "  def __init__(self, layers, embedding_dim):\n",
        "    super(Feedforward, self).__init__()\n",
        "\n",
        "    # get vocabulary list length from BERT tokenizer\n",
        "    vocab_size=len(tokenizer.vocab.keys())\n",
        "\n",
        "    # initialize embedding layer\n",
        "    self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # create list for NN layers and activation layers\n",
        "    self.fc = torch.nn.ModuleList()\n",
        "    self.activations = []\n",
        "\n",
        "    # append first layer\n",
        "    self.fc.append(torch.nn.Linear(embedding_dim*layers[0], layers[0]))\n",
        "    # self.fc.append(torch.nn.RNN(embedding_dim, layers[0]))\n",
        "    for i in range(len(layers)-1):\n",
        "      # Append activation function for previous layer\n",
        "      self.activations.append(torch.nn.ReLU())\n",
        "      # append layer with given dimensions\n",
        "      self.fc.append(torch.nn.Linear(layers[i], layers[i+1]))\n",
        "      print(layers[i], layers[i+1])\n",
        "\n",
        "    # # declare dropout layer function\n",
        "    # self.dropout = torch.nn.Dropout(0.2)\n",
        "            \n",
        "  def forward(self, x):\n",
        "    # Apply embeddding layer at beginnning and then train model\n",
        "    y = self.embeddings(x).squeeze()\n",
        "\n",
        "    for layer in range(len(self.fc)):\n",
        "      if layer!=len(self.fc)-1:\n",
        "        # Apply activation function for all layers other than the last\n",
        "        y= self.activations[layer](self.fc[layer](y))\n",
        "      else:\n",
        "        # No activation function since we are using BCEWithLogitsLoss\n",
        "        y= self.fc[layer](y)\n",
        "\n",
        "    return y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYd5FcsdwD3T"
      },
      "source": [
        "def evaluate_model(inputs, labels, data_name=\"\"):\n",
        "  \"\"\"\n",
        "  Model to evaluate model given inputs(text) and labels\n",
        "  :param inputs: TEXT to be classified\n",
        "  :param labels: Actual values of the LABELS to be predicted\n",
        "  :param data_name: Name of data being evaluated \n",
        "  \"\"\"\n",
        "  # Initiate model in evaluation mode\n",
        "  model.eval()\n",
        "  # Squeeze output to 1D\n",
        "  pred_labels = model(inputs)\n",
        "  # pred_labels = model(inputs).squeeze(1)\n",
        "  # pred_labels=torch.tensor(pred_labels)\n",
        "  pred_labels = (pred_labels>THRESHOLD).float()\n",
        "  # use criterion to calculate loss\n",
        "  before_train = criterion(pred_labels.squeeze(), labels)\n",
        "  print(f'{data_name}\\n\\tLoss: {before_train.item()}')\n",
        "  print(f\"\\tAccuracy: {accuracy_score(pred_labels, labels)}\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K7aIKx7pYcJ",
        "outputId": "753364d1-ce8c-4f55-8b4e-1d6f1652aed4"
      },
      "source": [
        "# Convert dataset 0s and 4s to 0s and 1s\n",
        "label_dict={0: 0, 4: 1}\n",
        "# Batch size for DataLoader. For fine-tuning BERT on a specific task, recommended size is 16 or 32\n",
        "batch_size = 64\n",
        "# hyper-parameters\n",
        "LEARNING_RATE=0.5\n",
        "THRESHOLD=0\n",
        "EPOCHS = 15\n",
        "\n",
        "df.LABEL= df.LABEL.replace(label_dict)\n",
        "train_inputs, train_labels,validation_inputs, validation_labels, test_inputs, test_labels = get_data_tensors(input_ids, df.LABEL, train_test_ratio=0.375, train_valid_ratio=0.3, max_input_length=max_input_length)\n",
        "print(f\"Number of training: {len(train_inputs)}\")\n",
        "print(f\"Number of validation: {len(validation_inputs)}\")\n",
        "print(f\"Number of testing: {len(test_inputs)}\")\n",
        "\n",
        "train_loader, validation_loader = get_loaders(train_inputs, train_labels, validation_inputs, validation_labels)\n",
        "model = Feedforward((max_input_length, 128, 32, 1),1)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr =LEARNING_RATE)\n",
        "\n",
        "# Pre-training evaluation\n",
        "evaluate_model(validation_inputs, validation_labels, data_name=\"Validation\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training: 35000\n",
            "Number of validation: 15000\n",
            "Number of testing: 30000\n",
            "64 128\n",
            "128 32\n",
            "32 1\n",
            "Validation\n",
            "\tLoss: 0.69467693567276\n",
            "\tAccuracy: 0.49773333333333336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP4DN-UowPbj",
        "outputId": "278b10c3-9591-4b3a-87f8-3713e28b40f7"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss=0\n",
        "    valid_loss=0\n",
        "    # model.train()\n",
        "    for data, target in train_loader:\n",
        "          # clear the gradients of all optimized variables\n",
        "          optimizer.zero_grad()\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the loss\n",
        "          loss = criterion(output.squeeze(), target)\n",
        "          # backward pass: compute gradient of the loss with respect to model parameters\n",
        "          loss.backward()\n",
        "          # perform a single optimization step (parameter update)\n",
        "          optimizer.step()\n",
        "          # update running training loss\n",
        "          train_loss += loss.item()*data.size(0)\n",
        "          \n",
        "    evaluate_model(validation_inputs, validation_labels, data_name=\"Validation\")\n",
        "    # for data, target in validation_loader:\n",
        "    #       # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    #       output = model(data)\n",
        "    #       # calculate the loss\n",
        "    #       output = (output>THRESHOLD).float()\n",
        "    #       loss = criterion(output.squeeze(1), target)\n",
        "    #       # update running validation loss\n",
        "    #       valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "    # print(f\"Epoch: {epoch+1}\")\n",
        "    # print(f\"\\tTrain loss: {train_loss/len(train_loader)}\")\n",
        "    # print(f\"\\tValidation loss: {valid_loss/len(validation_loader)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation\n",
            "\tLoss: 0.6879600882530212\n",
            "\tAccuracy: 0.6271333333333333\n",
            "Validation\n",
            "\tLoss: 0.6949470043182373\n",
            "\tAccuracy: 0.6302\n",
            "Validation\n",
            "\tLoss: 0.688076376914978\n",
            "\tAccuracy: 0.6302\n",
            "Validation\n",
            "\tLoss: 0.6890544295310974\n",
            "\tAccuracy: 0.6297333333333334\n",
            "Validation\n",
            "\tLoss: 0.6954717040061951\n",
            "\tAccuracy: 0.6308\n",
            "Validation\n",
            "\tLoss: 0.6865355968475342\n",
            "\tAccuracy: 0.6344666666666666\n",
            "Validation\n",
            "\tLoss: 0.6810457110404968\n",
            "\tAccuracy: 0.6359333333333334\n",
            "Validation\n",
            "\tLoss: 0.686265230178833\n",
            "\tAccuracy: 0.6356\n",
            "Validation\n",
            "\tLoss: 0.6880636811256409\n",
            "\tAccuracy: 0.6288\n",
            "Validation\n",
            "\tLoss: 0.6877918839454651\n",
            "\tAccuracy: 0.6324666666666666\n",
            "Validation\n",
            "\tLoss: 0.6884801387786865\n",
            "\tAccuracy: 0.6316666666666667\n",
            "Validation\n",
            "\tLoss: 0.6913681626319885\n",
            "\tAccuracy: 0.6342666666666666\n",
            "Validation\n",
            "\tLoss: 0.6946866512298584\n",
            "\tAccuracy: 0.63\n",
            "Validation\n",
            "\tLoss: 0.6971830129623413\n",
            "\tAccuracy: 0.6231333333333333\n",
            "Validation\n",
            "\tLoss: 0.6849522590637207\n",
            "\tAccuracy: 0.6306666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_LnZuuzwn7M",
        "outputId": "8c83195c-dffc-43e7-efa0-22b145975188"
      },
      "source": [
        "# Post-training evaluation\n",
        "evaluate_model(train_inputs, train_labels, data_name=\"Training\")\n",
        "evaluate_model(validation_inputs, validation_labels, data_name=\"Validation\")\n",
        "evaluate_model(test_inputs, test_labels, data_name=\"Testing\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "\tLoss: 0.597270667552948\n",
            "\tAccuracy: 0.8102571428571429\n",
            "Validation\n",
            "\tLoss: 0.6849522590637207\n",
            "\tAccuracy: 0.6306666666666667\n",
            "Testing\n",
            "\tLoss: 0.6833168864250183\n",
            "\tAccuracy: 0.6360666666666667\n"
          ]
        }
      ]
    }
  ]
}